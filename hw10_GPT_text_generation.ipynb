{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 10. Генерация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f21d5e",
      "metadata": {
        "id": "76f21d5e"
      },
      "source": [
        "### Задание 1 (8 баллов).\n",
        "\n",
        "Попробуйте дообучать GPT на каком-то другом тексте (можете попробовать любые стихи или какие-то специфичные вещи вроде анекдотов, теорий заговоров, постов в помоечных телеграм каналах, текстов журналистов и СМИ с выразительным стилем). \n",
        "Попробуйте разные методы и параметры генерации (beam search, температура, top_k и тп). Сохраните в тетрадке несколько хороших сгенерированных текстов\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "id": "nlz7yfKPLl3k"
      },
      "id": "nlz7yfKPLl3k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "oI-cXECJLk4g"
      },
      "id": "oI-cXECJLk4g",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2444e3fe",
      "metadata": {
        "id": "2444e3fe"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path, use_cache=False).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве данных для оубчения возьмем тексты песен исполнителя **Mujuice**"
      ],
      "metadata": {
        "id": "7azxKnefOMQA"
      },
      "id": "7azxKnefOMQA"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f07bd89b",
      "metadata": {
        "id": "f07bd89b"
      },
      "outputs": [],
      "source": [
        "with open('mujuice_texts.txt', 'r') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По сути, исходные тексты уже изначально похожи на сгенерированные...:"
      ],
      "metadata": {
        "id": "a1cDIhy2zwYq"
      },
      "id": "a1cDIhy2zwYq"
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[0:305])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKqg_FOsNYp1",
        "outputId": "6f853aa2-27cd-4eb1-8b8b-b0c7e4e387cf"
      },
      "id": "cKqg_FOsNYp1",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Дизели\n",
            " Ветер\n",
            " По Настроению\n",
            " Юный Вечер Убит\n",
            " Значит Я Жив\n",
            " Сердца Бит Долбит Вслед\n",
            " Пегасы И Скутеры\n",
            " Гонки Со Встречными\n",
            " Дела Как Дела\n",
            " Hola!\n",
            " Пьяные Феи\n",
            " *ah-ah*\n",
            " Пахнут Сиреневым\n",
            " Рубят Ресницами\n",
            " Головы С Плеч\n",
            " Течь\n",
            " Легко Было Ключ\n",
            " Проглотить И Пролиться\n",
            " Раскаленными Каплями\n",
            " Cool-Cool Death!\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание датасета\n",
        "train_dataset = TextDataset( tokenizer=tokenizer,file_path='mujuice_texts.txt',block_size=64, \n",
        "                            overwrite_cache=True)\n",
        "  \n",
        "# специальный класс который будет подавать в модель данные в нужном ей виде\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "SLckCuaVNp4A"
      },
      "id": "SLckCuaVNp4A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments( \n",
        "    output_dir= \"./finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=100, \n",
        "    per_device_train_batch_size=32, \n",
        "    per_device_eval_batch_size=32,  \n",
        "    gradient_accumulation_steps=16, \n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None) # Optimizer and lr scheduler\n",
        ")"
      ],
      "metadata": {
        "id": "Gj8YuAM5OBuk"
      },
      "id": "Gj8YuAM5OBuk",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "sFq4ol0COC9r"
      },
      "id": "sFq4ol0COC9r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(text, temperature=1, top_k=50, max_length=200):\n",
        "  input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      out = model.generate(input_ids, \n",
        "                          do_sample=True,\n",
        "                          temperature = temperature,\n",
        "                          top_k = top_k,\n",
        "                          max_length = max_length\n",
        "                          )\n",
        "  generated_text = list(map(tokenizer.decode, out))[0]\n",
        "  print()\n",
        "  print(generated_text)"
      ],
      "metadata": {
        "id": "iAeyG1TwQlSY"
      },
      "id": "iAeyG1TwQlSY",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**temperature ≈ 0**"
      ],
      "metadata": {
        "id": "YnT9M0GVowB7"
      },
      "id": "YnT9M0GVowB7"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Раз два три четыре пять \"\n",
        "generate_text(text, temperature=0.01, max_length=70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTcB-jGFo2HQ",
        "outputId": "891e6b83-749b-4062-9776-4e5c7a0e5612"
      },
      "id": "sTcB-jGFo2HQ",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Раз два три четыре пять \n",
            " Два три четыре пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять пять\n",
            " Три четыре пять\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**temperature = 1**"
      ],
      "metadata": {
        "id": "9JffV_mRpOxi"
      },
      "id": "9JffV_mRpOxi"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Раз два три четыре пять \"\n",
        "generate_text(text, temperature=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pwQi-2fkbdH",
        "outputId": "9cc938db-1402-4884-f414-306e477ac27a"
      },
      "id": "6pwQi-2fkbdH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Раз два три четыре пять  Звуками войны\n",
            " Я вижу огни\n",
            " Укрепим сталью льда\n",
            " Я верю в чудеса\n",
            " Зовёт с собой\n",
            " Холодный Восток и Атланты\n",
            " Мне бы снова оказаться в Париже\n",
            " О Боже, что за ерунда\n",
            " В темноте звезда в ладонь\n",
            " Я слышал это в толпе\n",
            " Мы спели там, на пляже\n",
            " Потанцуй с огнём снова\n",
            " Я жду ответа\n",
            " Не найдётся, где взять ответ\n",
            " Жди, дождёмся, вернёмся\n",
            " Вместе из огня и молний\n",
            " Будем танцевать ещё\n",
            " Вместе на одной широте\n",
            " Звёзд не вернуть в прах\n",
            " Звезды смотрят на Восток\n",
            " Мне кажется, что мы одни\n",
            " Оу… мы одни на свете!\n",
            " Оу! мы одни! Ура ура! ура ура ура ура! ура ура! ура ура ура ура! ура ура ура ура ура! и мы летим!\n",
            " Потанцуй с огнём снова\n",
            " Давай на время забыть о льдах\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**temperature = 0.75, top_k = 50**"
      ],
      "metadata": {
        "id": "WWSkElPQit0_"
      },
      "id": "WWSkElPQit0_"
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Раз два три четыре пять\", temperature=0.75, max_length=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWp9n1sXsSGU",
        "outputId": "326fa9bc-bfa1-448e-dbf7-40c986ba1845"
      },
      "id": "FWp9n1sXsSGU",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Раз два три четыре пять\n",
            " Ищи меня по всей планете\n",
            " В каждой клеточке твоей\n",
            " Все на нуле...\n",
            "\n",
            " Три копейки\n",
            " Семь раз отмерь\n",
            " Жизнь одна\n",
            " Мы умрем\n",
            " Но сможем ли мы снова\n",
            " Сжаться в кучку\n",
            " Потерять контроль\n",
            " На секунду\n",
            " На два\n",
            " Раз два три четыре пять\n",
            " Ищи меня по всей планете\n",
            " В каждой клеточке твоей\n",
            " Все на нуле...\n",
            "\n",
            " Три копейки\n",
            " Семь раз отмерь\n",
            " Жизнь\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Небо, звезды и луна\", temperature=0.75, max_length=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY2vrBV-qeOs",
        "outputId": "1f720f0d-1e9d-4df6-ccfc-407bb2dcbe47"
      },
      "id": "jY2vrBV-qeOs",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Небо, звезды и луна\n",
            " Не знаю, что с тобой\n",
            " Я знаю, что с тобой\n",
            " Оу!\n",
            "\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n",
            " I'm dead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**top_k=10**"
      ],
      "metadata": {
        "id": "4ZXC4O5quLMN"
      },
      "id": "4ZXC4O5quLMN"
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Небо, звезды и луна\", temperature=0.75, max_length=50, top_k=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHx7JZPJuNIu",
        "outputId": "c6fbebb5-a034-478e-ef84-52abab0fe869"
      },
      "id": "uHx7JZPJuNIu",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Небо, звезды и луна\n",
            " Я не знаю, где кончается лето\n",
            " Я не знаю, где начинается осень\n",
            " Я не знаю, где начинается весна\n",
            " Я не знаю, где кончается лето\n",
            " Я не знаю, где кончается осень\n",
            " Я не знаю\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настройка параметра num_beams не позволила получить какие-то разнообразные результаты:"
      ],
      "metadata": {
        "id": "nesPa0zNyyc9"
      },
      "id": "nesPa0zNyyc9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_beams = 10**"
      ],
      "metadata": {
        "id": "xf0ggkTHv3OJ"
      },
      "id": "xf0ggkTHv3OJ"
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(\"День и ночь\", return_tensors=\"pt\").to(DEVICE)\n",
        "out = model.generate(input_ids, do_sample=True, num_beams=10, temperature=0.75, max_length=40)\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bS1mlUfxMqb",
        "outputId": "9e04344e-08f7-4000-9c47-77a777f074fe"
      },
      "id": "1bS1mlUfxMqb",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "День и ночь\n",
            " Оу!\n",
            " Оу!\n",
            " Оу!\n",
            " Оу!\n",
            " Оу!\n",
            " Оу!\n",
            " Оу!\n",
            " Оу!\n",
            " Оу!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_beams = 2**"
      ],
      "metadata": {
        "id": "KGPxOrdzv6YK"
      },
      "id": "KGPxOrdzv6YK"
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(\"День и ночь\", return_tensors=\"pt\").to(DEVICE)\n",
        "out = model.generate(input_ids, do_sample=True, num_beams=2, temperature=0.75, max_length=40)\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjv1KpNQyFIj",
        "outputId": "bdb7e46e-f313-48dc-f4e1-f44619673d62"
      },
      "id": "Fjv1KpNQyFIj",
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "День и ночь\n",
            " Cool-cool\n",
            " Cool-cool\n",
            " Cool-cool\n",
            " Cool-cool\n",
            " Cool-cool\n",
            " Cool-cool\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8437e8",
      "metadata": {
        "id": "ae8437e8"
      },
      "source": [
        "### Задание  2 (2 балла)\n",
        "\n",
        "Ответьте на следующие вопросы:\n",
        "\n",
        "1) В каких статья были представлены GPT-1, GPT-2, GPT-3?\n",
        "\n",
        "GPT-1: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
        "<br>\n",
        "GPT-2: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "<br>\n",
        "GPT-3: [Language Models are Few-Shot Learnerslink text](https://arxiv.org/pdf/2005.14165.pdf)\n",
        "<br><br>\n",
        "2) Как собирался обучающий корпус для GPT-3? Каким образом создатели старались обеспечить высокое качество текстов в обучающей выборке?\n",
        "\n",
        "В качестве обучающих данных для модели GPT-3 использовался отфильтрованный корпус [Common Crawl](https://commoncrawl.org/). Датасет содержит петабайты данных в виде необработанных веб-страниц, сканированных с 2008 года. Разработчики GPT-3 взяли данные за период 2016-2019 годы. Помимо корпуса Common Crawl, из которого состояло 60% обучающей выборки, были также использованы датасеты WebText2, Books1, Books2 и Wikipedia.\n",
        "<br><br>\n",
        "Проблема корпуса Common Crawl, с которой столкнулись разработчики GPT-3, заключалась разном уровне качества текстов. Авторы решили проблему, отфильтровав корпус с помощью следующих техник:\n",
        "1. Корпус CommonCrawl был пропущен через классификатор текстов по качеству. Чтобы обучить такой классификатор, в качестве позитивных примеров были взяты тексты из готовых отфильтрованных датасетов (Wikipedia, WebText, Books), в качестве отрицательных — неотфильтрованный датасет CommonCrawl.\n",
        "2. Чтобы исключить из корпуса избытночные данные и предотвратить переобучение, разработчики применили дедупликацию данных (как и внутри каждого документа, так и на уровне всего корпуса).\n",
        "3. Чтобы еще больше повысить разнообразие текстов, авторы расширили корпус CommonCrawl, добавив в обучающую выборку корпусы Wikipedia, WebText2 и Books.\n",
        "\n",
        "Размер CommonCrawl 2016-2019 до фильтрации: 45 TB\n",
        "<br>\n",
        "После фильтрации: 570GB"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "hw10_GPT_text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}