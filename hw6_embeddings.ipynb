{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6af66482",
      "metadata": {
        "id": "6af66482"
      },
      "source": [
        "#### Важное требование ко всей домашке в целом: в jupyter ноутбуке не должно был лишнего кода (т.е. если вы взяли за основу семинар, не забудьте удалить все лишнее)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "1aed5eae",
      "metadata": {
        "id": "1aed5eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff06e513-eb41-4fe5-c8c3-5b44a7be937d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from pymystem3 import Mystem\n",
        "import json\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "! pip install pymorphy2\n",
        "from pymorphy2 import MorphAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Корпус с Wikipedia в качестве обучающих данных"
      ],
      "metadata": {
        "id": "ayzkIMKC8so9"
      },
      "id": "ayzkIMKC8so9"
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_texts = open('wiki_data.txt').read().split('\\n')"
      ],
      "metadata": {
        "id": "b6omBBP48sQ1"
      },
      "id": "b6omBBP48sQ1",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы по несколько раз не делать долгую предобработку с лемматизацией, предобработанные тексты были сохранены в pickle файл"
      ],
      "metadata": {
        "id": "WLzL5t2en7dm"
      },
      "id": "WLzL5t2en7dm"
    },
    {
      "cell_type": "code",
      "source": [
        "with open('wiki_texts_preprocessed.pickle', 'rb') as f:\n",
        "    wiki_texts_preprocessed = pickle.load(f)"
      ],
      "metadata": {
        "id": "J2uUjicxTAi1"
      },
      "id": "J2uUjicxTAi1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_texts_preprocessed[0][45:50] # слова приведены к нормальной форме"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPU37es1zv45",
        "outputId": "01468b54-a57c-4030-dbc1-cfceae428cfc"
      },
      "id": "jPU37es1zv45",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['соединить', 'асфальтовый', 'дорога', 'с', 'посёлок']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c422aa0",
      "metadata": {
        "id": "9c422aa0"
      },
      "source": [
        "# Задание 1 (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a72790",
      "metadata": {
        "id": "e4a72790"
      },
      "source": [
        "Обучите word2vec модели с негативным семплированием (cbow и skip-gram) с помощью tensorflow аналогично тому, как это было сделано в семинаре. Вам нужно изменить следующие пункты: \n",
        "1) добавьте лемматизацию в предобработку (любым способом)  \n",
        "2) измените размер окна на 6 для cbow и 12 для skip gram (обратите внимание, что размер окна = #слов слева + #слов справа, в gen_batches в семинаре window не так используется)  \n",
        "3) измените часть с np.random.randint(vocab_size) так, чтобы случайные негативные примеры выбирались обратно пропорционально частотностям слов (частотные должны выбираться реже, а редкие чаще)\n",
        "\n",
        "Выберете несколько не похожих по смыслу слов, и протестируйте полученные эмбединги (найдите ближайшие слова и оцените правильность, как в семинаре)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# поиск векторов с минимальным косинусным расстоянием\n",
        "def most_similar(word, embeddings):\n",
        "    similar = [id2word[i] for i in \n",
        "               cosine_distances(embeddings[word2id[word]].reshape(1, -1), embeddings).argsort()[0][:10]]\n",
        "    return similar"
      ],
      "metadata": {
        "id": "WM78VkochoXv"
      },
      "id": "WM78VkochoXv",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph = MorphAnalyzer()\n",
        "\n",
        "# предобработка -> токенизация + лемматизация\n",
        "def preprocess(text):\n",
        "  tokens = text.lower().split()\n",
        "  tokens = [token.strip(punctuation) for token in tokens]\n",
        "  # лемматизируем с pymorphy\n",
        "  tokens = [morph.parse(token)[0].normal_form for token in tokens]\n",
        "  return(tokens)"
      ],
      "metadata": {
        "id": "6SUzocPu_Kt6"
      },
      "id": "6SUzocPu_Kt6",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in wiki_texts_preprocessed:\n",
        "    vocab.update(text)"
      ],
      "metadata": {
        "id": "HauaOJjhZWb0"
      },
      "id": "HauaOJjhZWb0",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 30:\n",
        "        filtered_vocab.add(word)"
      ],
      "metadata": {
        "id": "FNLZRnT1Z2ut"
      },
      "id": "FNLZRnT1Z2ut",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfYLaqZDaKDb",
        "outputId": "89dc28e7-a1ef-49b8-a96a-f5512169694e"
      },
      "id": "WfYLaqZDaKDb",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11987"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем словарь с индексами слов"
      ],
      "metadata": {
        "id": "X-LxKJ99ayxM"
      },
      "id": "X-LxKJ99ayxM"
    },
    {
      "cell_type": "code",
      "source": [
        "word2id = { 'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "    \n",
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "metadata": {
        "id": "qr7CfpfqaKJW"
      },
      "id": "qr7CfpfqaKJW",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "\n",
        "for text in wiki_texts_preprocessed:\n",
        "    tokens = text\n",
        "    ids = [word2id[token] for token in tokens if token in word2id]\n",
        "    sentences.append(ids)"
      ],
      "metadata": {
        "id": "LUeVNvB-aeKl"
      },
      "id": "LUeVNvB-aeKl",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим словарь из пар *id_слова + частота*, чтобы учитывать частоту слов при выборе отрицательных примеров для negative sampling:"
      ],
      "metadata": {
        "id": "oUaXs9sOxmex"
      },
      "id": "oUaXs9sOxmex"
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_vocab_counts = {}\n",
        "\n",
        "for key in vocab.keys():\n",
        "    if key in filtered_vocab:\n",
        "      filtered_vocab_counts[key] = vocab[key]"
      ],
      "metadata": {
        "id": "jLTYV-vRreLt"
      },
      "id": "jLTYV-vRreLt",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(filtered_vocab_counts.items())[150:155]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b23B1KIdyvfI",
        "outputId": "d2c4caac-ed0d-404a-93c4-fa8762a48a8b"
      },
      "id": "b23B1KIdyvfI",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('выделить', 628),\n",
              " ('относительно', 443),\n",
              " ('крупный', 2381),\n",
              " ('весь', 7838),\n",
              " ('обозначить', 172)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_vocab_probs_ids= {}\n",
        "\n",
        "n = len(filtered_vocab_counts)\n",
        "for key in filtered_vocab_counts:\n",
        "  if key in word2id.keys():\n",
        "      filtered_vocab_probs_ids[word2id[key]] = filtered_vocab_counts[key]/n"
      ],
      "metadata": {
        "id": "nbVq1-_DrmBl"
      },
      "id": "nbVq1-_DrmBl",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(filtered_vocab_probs_ids.items())[150:155]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sppXK5HKykbm",
        "outputId": "791cb344-2b68-4e33-9ace-7f6c5e636291"
      },
      "id": "sppXK5HKykbm",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(10779, 0.05239008926336865),\n",
              " (2074, 0.036956703095019604),\n",
              " (197, 0.1986318511721031),\n",
              " (7031, 0.6538750312838909),\n",
              " (11804, 0.014348877951113706)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip Gram Negative Sampling"
      ],
      "metadata": {
        "id": "XK8JSAMnbXtS"
      },
      "id": "XK8JSAMnbXtS"
    },
    {
      "cell_type": "code",
      "source": [
        "# skip gram, генерируем обучающие примеры батчами\n",
        "def gen_batches_sg(sentences, window, batch_size=1000):\n",
        "    \n",
        "    while True:\n",
        "        X_target = []\n",
        "        X_context = []\n",
        "        y = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            for i in range(len(sent)-1):\n",
        "                word = sent[i]\n",
        "\n",
        "                # задаем контекст, делим размер окна на 2 \n",
        "                context = sent[max(0, i-window//2):i] + sent[i+1:i+window//2]\n",
        "\n",
        "                for context_word in context:\n",
        "                    # генерируем положительные примеры\n",
        "                    X_target.append(word)\n",
        "                    X_context.append(context_word)\n",
        "                    y.append(1)\n",
        "                    \n",
        "                    # генерируем отрицательные примеры\n",
        "                    X_target.append(word)    \n",
        "                    # генерируем 50 рандомных индексов с учетом частот\n",
        "                    id_based_on_frequency = random.choices(list(filtered_vocab_probs_ids.keys()), \n",
        "                                                           weights=filtered_vocab_probs_ids.values(), \n",
        "                                                           k=50)[np.random.randint(50)] # берем один рандомный индекс из 50-ти\n",
        "                    X_context.append(id_based_on_frequency) \n",
        "                    y.append(0)\n",
        "                    \n",
        "                    if len(X_target) >= batch_size:\n",
        "                        X_target = np.array(X_target)\n",
        "                        X_context = np.array(X_context)\n",
        "                        y = np.array(y)\n",
        "                        yield ((X_target, X_context), y)\n",
        "                        X_target = []\n",
        "                        X_context = []\n",
        "                        y = []"
      ],
      "metadata": {
        "id": "b5Pkecf8eIMJ"
      },
      "id": "b5Pkecf8eIMJ",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_target = tf.keras.layers.Input(shape=(1,)) # input для целевого слова\n",
        "inputs_context = tf.keras.layers.Input(shape=(1,)) # input для контекстного слова\n",
        "\n",
        "\n",
        "embeddings_target = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_target, )\n",
        "embeddings_context = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_context, )\n",
        "\n",
        "target = tf.keras.layers.Flatten()(embeddings_target)\n",
        "context = tf.keras.layers.Flatten()(embeddings_context)\n",
        "\n",
        "dot = tf.keras.layers.Dot(1)([target, context])\n",
        "outputs = tf.keras.layers.Activation(activation='sigmoid')(dot)\n",
        "\n",
        "skipgram_neg_smp_model = tf.keras.Model(inputs=[inputs_target, inputs_context], \n",
        "                       outputs=outputs)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "skipgram_neg_smp_model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "aclRI5uQeneP"
      },
      "id": "aclRI5uQeneP",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_neg_smp_model.fit(gen_batches_sg(sentences[:19000], window=12),\n",
        "          validation_data=gen_batches_sg(sentences[19000:],  window=12),\n",
        "          batch_size=1000,\n",
        "          steps_per_epoch=3000,\n",
        "          validation_steps=30,\n",
        "          epochs=2)"
      ],
      "metadata": {
        "id": "_wahtg_zzunh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87b84ee-760a-458a-a589-684297a9ae24"
      },
      "id": "_wahtg_zzunh",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "3000/3000 [==============================] - 1070s 357ms/step - loss: 0.6764 - accuracy: 0.5577 - val_loss: 0.6749 - val_accuracy: 0.5709\n",
            "Epoch 2/2\n",
            "3000/3000 [==============================] - 1123s 374ms/step - loss: 0.6637 - accuracy: 0.5829 - val_loss: 0.6831 - val_accuracy: 0.5540\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f53712e8b90>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_skipgram_negsmplg =  skipgram_neg_smp_model.layers[2].get_weights()[0]"
      ],
      "metadata": {
        "id": "KIuuIV7vjMKh"
      },
      "id": "KIuuIV7vjMKh",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar('город', embeddings_skipgram_negsmplg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBebytTZjYr4",
        "outputId": "4d99866b-04b7-415e-d8ee-ff63d4c85ab3"
      },
      "id": "zBebytTZjYr4",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['город',\n",
              " 'скотсдейл',\n",
              " 'провинция',\n",
              " 'северо-запад',\n",
              " 'онтарио',\n",
              " 'расположить',\n",
              " 'деревня',\n",
              " 'напрямую',\n",
              " 'полуостров',\n",
              " 'район']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar('школа', embeddings_skipgram_negsmplg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt1eekhZjbjD",
        "outputId": "ae8cbf74-1c7a-4b48-ed8d-8b3d179179dd"
      },
      "id": "Qt1eekhZjbjD",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['школа',\n",
              " 'учиться',\n",
              " 'окончить',\n",
              " 'бакалавр',\n",
              " 'преподавать',\n",
              " 'университет',\n",
              " 'училище',\n",
              " 'московский',\n",
              " 'факультет',\n",
              " 'поступить']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBOW negative sampling"
      ],
      "metadata": {
        "id": "XlwEPrZg0Jqv"
      },
      "id": "XlwEPrZg0Jqv"
    },
    {
      "cell_type": "code",
      "source": [
        "# cbow \n",
        "def gen_batches_cbow(sentences, window, batch_size=1000):\n",
        "    while True:\n",
        "        X_target = []\n",
        "        X_context = []\n",
        "        y = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            for i in range(len(sent)-1):\n",
        "                word = sent[i]\n",
        "                # делим размер окна на 2\n",
        "                context = sent[max(0, i-window//2):i] + sent[i+1:i+window//2]\n",
        "\n",
        "                X_target.append(word)\n",
        "                X_context.append(context)\n",
        "                y.append(1)\n",
        "                \n",
        "                id_based_on_frequency = random.choices(list(filtered_vocab_probs_ids.keys()), \n",
        "                                                           weights=filtered_vocab_probs_ids.values(), \n",
        "                                                           k=50)[np.random.randint(50)]\n",
        "                X_target.append(np.random.randint(id_based_on_frequency))\n",
        "                X_context.append(context)\n",
        "                y.append(0)\n",
        "\n",
        "                if len(X_target) == batch_size:\n",
        "                    X_target = np.array(X_target)\n",
        "                    X_context = tf.keras.preprocessing.sequence.pad_sequences(X_context, maxlen=window*2)\n",
        "                    y = np.array(y)\n",
        "                    yield ((X_target, X_context), y)\n",
        "                    X_target = []\n",
        "                    X_context = []\n",
        "                    y = []"
      ],
      "metadata": {
        "id": "t32U5WrB0M6m"
      },
      "id": "t32U5WrB0M6m",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cbow negative sampling\n",
        "inputs_target = tf.keras.layers.Input(shape=(1,))\n",
        "inputs_context = tf.keras.layers.Input(shape=(10,))\n",
        "\n",
        "\n",
        "embeddings_target = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_target, )\n",
        "embeddings_context = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=300)(inputs_context, )\n",
        "\n",
        "target = tf.keras.layers.Flatten()(embeddings_target)\n",
        "context = tf.keras.layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=1))(embeddings_context)\n",
        "dot = tf.keras.layers.Dot(1)([target, context])\n",
        "\n",
        "outputs = tf.keras.layers.Activation(activation='sigmoid')(dot)\n",
        "\n",
        "model = tf.keras.Model(inputs=[inputs_target, inputs_context], \n",
        "                       outputs=outputs)\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mm13BsFM0c7v"
      },
      "id": "mm13BsFM0c7v",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(gen_batches_cbow(sentences[:19000], window=6),\n",
        "          validation_data=gen_batches_cbow(sentences[19000:],  window=6),\n",
        "          batch_size=1000,\n",
        "          steps_per_epoch=3000,\n",
        "          validation_steps=30,\n",
        "          epochs=2)"
      ],
      "metadata": {
        "id": "EvLG_XB6EWRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff33256-0afc-4a9b-f45f-094644e12aea"
      },
      "id": "EvLG_XB6EWRc",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "3000/3000 [==============================] - 1028s 343ms/step - loss: 0.3343 - accuracy: 0.8572 - val_loss: 0.2957 - val_accuracy: 0.8777\n",
            "Epoch 2/2\n",
            "3000/3000 [==============================] - 1033s 344ms/step - loss: 0.2732 - accuracy: 0.8885 - val_loss: 0.2552 - val_accuracy: 0.8954\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5373488490>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_cbow_negsmplg = model.layers[2].get_weights()[0]"
      ],
      "metadata": {
        "id": "2tcFQnvXgylU"
      },
      "id": "2tcFQnvXgylU",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar('город', embeddings_cbow_negsmplg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEKew7jairOr",
        "outputId": "d39576bc-0f33-47ed-992c-11945747d80e"
      },
      "id": "uEKew7jairOr",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['город',\n",
              " 'район',\n",
              " 'посёлок',\n",
              " 'улица',\n",
              " 'ростов',\n",
              " 'здание',\n",
              " 'монастырь',\n",
              " 'столица',\n",
              " 'деревня',\n",
              " 'финляндия']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar('школа', embeddings_cbow_negsmplg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkWXoREki5Ku",
        "outputId": "e4e1c66c-3f79-4b72-965b-1908468ee2a0"
      },
      "id": "fkWXoREki5Ku",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['школа',\n",
              " 'училище',\n",
              " 'университет',\n",
              " 'институт',\n",
              " 'семинария',\n",
              " 'мастерская',\n",
              " 'учиться',\n",
              " 'учитель',\n",
              " 'музей',\n",
              " 'колледж']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод**: если судить по полученным похожим словам, обе модели генерируют достаточно хорошие эмбеддинги. Функция потерь для модели CBOW минимизировалась быстрее, чем для Skip-Gram (при равном числе эпох и размере батча). Это может быть связано с размером окна, который для CBOW был в два раза меньше, чем у Skip-Gram (но лучше проверить)"
      ],
      "metadata": {
        "id": "qpd_Sh-wvtx5"
      },
      "id": "qpd_Sh-wvtx5"
    },
    {
      "cell_type": "markdown",
      "id": "c3b61b7c",
      "metadata": {
        "id": "c3b61b7c"
      },
      "source": [
        "# Задание 2 (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66eff080",
      "metadata": {
        "id": "66eff080"
      },
      "source": [
        "Обучите 1 word2vec и 1 fastext модель в gensim. В каждой из модели нужно задать все параметры, которые мы разбирали на семинаре. Заданные значения должны отличаться от дефолтных и от тех, что мы использовали на семинаре."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# texts = [preprocess(text) for text in wiki_texts]"
      ],
      "metadata": {
        "id": "p7MuzivNESaO"
      },
      "id": "p7MuzivNESaO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = wiki_texts_preprocessed"
      ],
      "metadata": {
        "id": "jNIz6f1BnNoa"
      },
      "id": "jNIz6f1BnNoa",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec"
      ],
      "metadata": {
        "id": "KCN5piQVGR8e"
      },
      "id": "KCN5piQVGR8e"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "w2v = gensim.models.Word2Vec(texts, \n",
        "                             size=128, \n",
        "                             min_count=25, \n",
        "                             max_vocab_size=11000,\n",
        "                             window=7,\n",
        "                             iter=8,\n",
        "                             sg=1,\n",
        "                             hs=0,\n",
        "                             negative=10,\n",
        "                             sample=1e-4,\n",
        "                             ns_exponent=0.5,\n",
        "                             cbow_mean=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0usZk8C5Ei3H",
        "outputId": "dd4ccf35-7c16-47aa-f753-86497d30e9c5"
      },
      "id": "0usZk8C5Ei3H",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5min 7s, sys: 1.18 s, total: 5min 8s\n",
            "Wall time: 2min 56s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar('город')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOANS1aKGPYl",
        "outputId": "928d5cb9-e87a-465c-a5a0-d275862e8254"
      },
      "id": "WOANS1aKGPYl",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('столица', 0.7060852646827698),\n",
              " ('центр', 0.6457962989807129),\n",
              " ('посёлок', 0.623887300491333),\n",
              " ('район', 0.5897524356842041),\n",
              " ('городок', 0.5703724026679993),\n",
              " ('городской', 0.5620348453521729),\n",
              " ('коммуна', 0.5583091974258423),\n",
              " ('расположить', 0.5504782199859619),\n",
              " ('пункт', 0.5489876866340637),\n",
              " ('административный', 0.5486613512039185)]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.wv.most_similar('школа')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCC8b28JnWJj",
        "outputId": "2a4fcb11-f9bf-41e3-c14c-5d497f515667"
      },
      "id": "mCC8b28JnWJj",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('учиться', 0.7955573201179504),\n",
              " ('обучаться', 0.7156641483306885),\n",
              " ('училище', 0.7023470401763916),\n",
              " ('учитель', 0.6892426013946533),\n",
              " ('окончить', 0.6658641695976257),\n",
              " ('колледж', 0.6552792191505432),\n",
              " ('обучение', 0.6394620537757874),\n",
              " ('преподаватель', 0.629569411277771),\n",
              " ('преподавать', 0.627708911895752),\n",
              " ('факультет', 0.6234071254730225)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FastText"
      ],
      "metadata": {
        "id": "949YkENrGF_m"
      },
      "id": "949YkENrGF_m"
    },
    {
      "cell_type": "code",
      "source": [
        "ft = gensim.models.FastText(texts, min_n=4, max_n=9)"
      ],
      "metadata": {
        "id": "99AiPIqmGHYF"
      },
      "id": "99AiPIqmGHYF",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для FastText модели можно сразу заметить влияние n-грамм:"
      ],
      "metadata": {
        "id": "cZFKPgXLuW5n"
      },
      "id": "cZFKPgXLuW5n"
    },
    {
      "cell_type": "code",
      "source": [
        "ft.wv.most_similar('город')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMNtX0ZoGLj-",
        "outputId": "ae0c8cbb-a5f7-48b4-d43d-e6b8b5f9e130"
      },
      "id": "gMNtX0ZoGLj-",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ужгород', 0.9822290539741516),\n",
              " ('городе»', 0.9706169366836548),\n",
              " ('«город', 0.9700791835784912),\n",
              " ('город»', 0.9697728157043457),\n",
              " ('горо', 0.9651158452033997),\n",
              " ('городец', 0.9648722410202026),\n",
              " ('горох', 0.9550338387489319),\n",
              " ('городов', 0.9530773758888245),\n",
              " ('огород', 0.9500880241394043),\n",
              " ('белгород', 0.9493427872657776)]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft.wv.most_similar('школа')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug545JYOnY9P",
        "outputId": "cf0bb462-f7b1-4791-f4e1-c382056b09d9"
      },
      "id": "Ug545JYOnY9P",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('школа»', 0.9956318736076355),\n",
              " ('«школа', 0.9949550628662109),\n",
              " ('школы»', 0.9881325960159302),\n",
              " ('эркола', 0.9765649437904358),\n",
              " ('школе»', 0.9650436639785767),\n",
              " ('анкола', 0.9616413116455078),\n",
              " ('кока-кола', 0.9542515873908997),\n",
              " ('бизнес-школа', 0.9448583722114563),\n",
              " ('профтехшкола', 0.9263396263122559),\n",
              " ('пенсакола', 0.9237112998962402)]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод:** в данном случае, для модели Word2Vec получились более качественные эмбеддинги. Для FastText была взята слишком большая длина n-gram (минимум 4 символа), из-за чего модель стала просто находить однокоренные слова, между которыми может совсем отсутствовать связь: город -> горох, школа -> кока-кола..."
      ],
      "metadata": {
        "id": "zPiz8U73ujd_"
      },
      "id": "zPiz8U73ujd_"
    },
    {
      "cell_type": "markdown",
      "id": "e4bb928c",
      "metadata": {
        "id": "e4bb928c"
      },
      "source": [
        "# Задание 3 (4 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3019b0d1",
      "metadata": {
        "id": "3019b0d1"
      },
      "source": [
        "Используя датасет для классификации (labeled.csv) и простую нейронную сеть (последняя модель в семинаре), оцените качество полученных эмбедингов в задании 1 и 2 (4 набора эмбедингов), также проверьте 1 любую из предобученных моделей с rus-vectores (но только не tayga_upos_skipgram_300_2_2019). \n",
        "Какая модель показывает наилучший результат?\n",
        "\n",
        "Убедитесь, что для каждой модели вы корректно воспроизводите пайплайн предобработки (в 1 задании у вас лемматизация, не забудьте ее применить к датасету для классификации; у выбранной предобученной модели может быть своя специфичная предобработка - ее нужно воспроизвести)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Предобработка"
      ],
      "metadata": {
        "id": "n1WtuH5bs6s1"
      },
      "id": "n1WtuH5bs6s1"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "ed908832",
      "metadata": {
        "id": "ed908832"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('labeled.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "60c18c5a",
      "metadata": {
        "id": "60c18c5a"
      },
      "outputs": [],
      "source": [
        "# берем предобработку из задания 1\n",
        "data['norm_text'] = data.comment.apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in data['norm_text']:\n",
        "    vocab.update(text)\n",
        "    \n",
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 5:\n",
        "        filtered_vocab.add(word)\n",
        "\n",
        "len(filtered_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd2jRmwNtD8f",
        "outputId": "4dfe8bdd-5631-465c-9df9-54d93b0e057b"
      },
      "id": "dd2jRmwNtD8f",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6309"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2id2 = { 'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id2[word] = len(word2id2)\n",
        "id2word2 = {i:word for word, i in word2id2.items()}"
      ],
      "metadata": {
        "id": "YdIAxnJytcrF"
      },
      "id": "YdIAxnJytcrF",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Переводим слова в индексы"
      ],
      "metadata": {
        "id": "CZrAyj9ut_UE"
      },
      "id": "CZrAyj9ut_UE"
    },
    {
      "cell_type": "code",
      "source": [
        "X_glob = []\n",
        "\n",
        "for tokens in data['norm_text']:\n",
        "    ids = [word2id2[token] for token in tokens if token in word2id2]\n",
        "    X_glob.append(ids)"
      ],
      "metadata": {
        "id": "Pw-vUB_EuByh"
      },
      "id": "Pw-vUB_EuByh",
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classification with custom embeddings"
      ],
      "metadata": {
        "id": "dJ1nxAd6qH-8"
      },
      "id": "dJ1nxAd6qH-8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пайплан для моделей из Gensim (задание 2):"
      ],
      "metadata": {
        "id": "HO65B4Hf5p7v"
      },
      "id": "HO65B4Hf5p7v"
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_with_embeddings_gensim(word2id, emb_model, vec_size):\n",
        "  X = tf.keras.preprocessing.sequence.pad_sequences(X_glob, maxlen=vec_size)\n",
        "  y = data.toxic.values\n",
        "\n",
        "  # разбиваем датасет на обучающую и тестовую выборки\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05)\n",
        "  # создаем матрицу с векторными представлениями\n",
        "  weights = np.zeros((len(word2id), vec_size))\n",
        "\n",
        "  for word, i in word2id.items():\n",
        "      # вектор паддинга оставим нулевым\n",
        "      if word == 'PAD':\n",
        "          continue\n",
        "      try:\n",
        "          weights[i] = emb_model.wv[word]\n",
        "      \n",
        "      \n",
        "      except KeyError:\n",
        "          # для слов, которых нет в модели возьмем  рандомный вектор\n",
        "          continue\n",
        "          weights[i] = emb_model.wv['ллалалаллала']\n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=(vec_size,))\n",
        "\n",
        "  # передаем матрицу в эмбединг слой\n",
        "  embeddings = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=vec_size, \n",
        "                                        trainable=False,\n",
        "                                        weights=[weights])(inputs, )\n",
        "  mean = tf.keras.layers.Lambda(lambda x: tf.keras.backend.mean(x,  axis=1))(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(mean)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # обучаем модель\n",
        "  model.fit(X_train, y_train, \n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=32,\n",
        "         epochs=30)\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "NcumPzVXuzrj"
      },
      "id": "NcumPzVXuzrj",
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пайплайн для моделей на TensoFlow (задание 1)"
      ],
      "metadata": {
        "id": "XM4NBquY6Mbo"
      },
      "id": "XM4NBquY6Mbo"
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_with_embeddings_tf(id2word, emb_model, vec_size):\n",
        "  X = tf.keras.preprocessing.sequence.pad_sequences(X_glob, maxlen=vec_size)\n",
        "  y = data.toxic.values\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05)\n",
        "  weights = np.zeros((len(word2id), vec_size))\n",
        "\n",
        "  # получаем веса из последнего слоя TF модели (в нашем случае берем 2-й слой)\n",
        "  model_weights = emb_model.layers[2].get_weights()[0]\n",
        "\n",
        "  # тут используем id2word, тк вытаскиваем вектора из tf-модели по id \n",
        "  for id in id2word.keys(): \n",
        "      if id == 0:\n",
        "          continue\n",
        "      try:\n",
        "          weights[id] = model_weights[id]\n",
        "      \n",
        "      \n",
        "      except KeyError:\n",
        "          # для слов, которых нет в модели возьмем  рандомный вектор\n",
        "          continue\n",
        "          weights[id] = model_weights[42424242424242] \n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=(vec_size,))\n",
        "\n",
        "  # передаем матрицу в эмбединг слой\n",
        "  embeddings = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=vec_size, \n",
        "                                        trainable=False,\n",
        "                                        weights=[weights])(inputs, )\n",
        "  mean = tf.keras.layers.Lambda(lambda x: tf.keras.backend.mean(x,  axis=1))(embeddings)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(mean)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # обучаем модель\n",
        "  model.fit(X_train, y_train, \n",
        "          validation_data=(X_valid, y_valid),\n",
        "          batch_size=32,\n",
        "         epochs=30)\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "4-b0hpTS6TDQ"
      },
      "id": "4-b0hpTS6TDQ",
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Сравнение полученных классификаторов"
      ],
      "metadata": {
        "id": "We817Kpgqdqm"
      },
      "id": "We817Kpgqdqm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Classifier | Accuracy on final epoch |\n",
        "| --- | --- |\n",
        "| Fasttext Classifier | 0.72 |\n",
        "| Word2Vec Classifier | 0.69 |\n",
        "| CBOW Negative Sampling | 0.6653 |\n",
        "| Skip-gram Negative Sampling | 0.6656 |\n"
      ],
      "metadata": {
        "id": "MY2LlFh7yZTv"
      },
      "id": "MY2LlFh7yZTv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Не совсем понятно, почему 3 и 4 модели (CBOW и Skip-gram из задания 1) показали почти одинаковые результаты при классификации, может что-то не так сделала в *classify_with_embeddings_tf()*"
      ],
      "metadata": {
        "id": "l2SQ9vbM_U6O"
      },
      "id": "l2SQ9vbM_U6O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FastText classifier"
      ],
      "metadata": {
        "id": "3BX184irCz_O"
      },
      "id": "3BX184irCz_O"
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_classifier = classify_with_embeddings_gensim(word2id2, ft, 100)"
      ],
      "metadata": {
        "id": "p3z1l6ZVweoe"
      },
      "id": "p3z1l6ZVweoe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_classifier.history.history['accuracy'][29]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqyX9LZGCgWu",
        "outputId": "3f277b8f-11fd-44d6-ed07-e2b788ac2829"
      },
      "id": "OqyX9LZGCgWu",
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7199620008468628"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word2Vec Classifier"
      ],
      "metadata": {
        "id": "Gv3PCRLqCrWj"
      },
      "id": "Gv3PCRLqCrWj"
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_classifier = classify_with_embeddings_gensim(word2id2, w2v, 128)"
      ],
      "metadata": {
        "id": "UFj5rwqOwzq2"
      },
      "id": "UFj5rwqOwzq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_classifier.history.history['accuracy'][29]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpAbj6ivCbGq",
        "outputId": "0fc4eaaf-4a91-4832-fbb6-166293006d41"
      },
      "id": "KpAbj6ivCbGq",
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6998758316040039"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CBOW Negative Sampling Classifier"
      ],
      "metadata": {
        "id": "4bEXlUj_BKT-"
      },
      "id": "4bEXlUj_BKT-"
    },
    {
      "cell_type": "code",
      "source": [
        "id2word"
      ],
      "metadata": {
        "id": "eVXHNqYk7184"
      },
      "id": "eVXHNqYk7184",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_classifier = classify_with_embeddings_tf(id2word2, model, 300)"
      ],
      "metadata": {
        "id": "qMA2MZAPAind"
      },
      "id": "qMA2MZAPAind",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_classifier.history.history['accuracy'][29]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJHJTNQVCPl6",
        "outputId": "3b08d64b-a3f1-46e0-8565-a112131f2da8"
      },
      "id": "aJHJTNQVCPl6",
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.665254533290863"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Skip-gram Negative Sampling Classifier"
      ],
      "metadata": {
        "id": "iX6ma-4RmVrK"
      },
      "id": "iX6ma-4RmVrK"
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_classifier = classify_with_embeddings_tf(id2word2, skipgram_neg_smp_model, 300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TCzGFRPmuup",
        "outputId": "56fdbc00-8ca8-4191-bbf7-ddde1f13a70f"
      },
      "id": "-TCzGFRPmuup",
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "428/428 [==============================] - 4s 7ms/step - loss: 0.6629 - accuracy: 0.6655 - val_loss: 0.6498 - val_accuracy: 0.6560\n",
            "Epoch 2/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6365 - accuracy: 0.6656 - val_loss: 0.6414 - val_accuracy: 0.6560\n",
            "Epoch 3/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6302 - accuracy: 0.6656 - val_loss: 0.6404 - val_accuracy: 0.6560\n",
            "Epoch 4/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6283 - accuracy: 0.6656 - val_loss: 0.6405 - val_accuracy: 0.6560\n",
            "Epoch 5/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6274 - accuracy: 0.6656 - val_loss: 0.6404 - val_accuracy: 0.6560\n",
            "Epoch 6/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6266 - accuracy: 0.6656 - val_loss: 0.6404 - val_accuracy: 0.6560\n",
            "Epoch 7/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6259 - accuracy: 0.6656 - val_loss: 0.6404 - val_accuracy: 0.6560\n",
            "Epoch 8/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6254 - accuracy: 0.6656 - val_loss: 0.6403 - val_accuracy: 0.6560\n",
            "Epoch 9/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6248 - accuracy: 0.6656 - val_loss: 0.6403 - val_accuracy: 0.6560\n",
            "Epoch 10/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6243 - accuracy: 0.6656 - val_loss: 0.6402 - val_accuracy: 0.6560\n",
            "Epoch 11/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6238 - accuracy: 0.6656 - val_loss: 0.6401 - val_accuracy: 0.6560\n",
            "Epoch 12/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6234 - accuracy: 0.6656 - val_loss: 0.6400 - val_accuracy: 0.6560\n",
            "Epoch 13/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6230 - accuracy: 0.6656 - val_loss: 0.6401 - val_accuracy: 0.6560\n",
            "Epoch 14/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6226 - accuracy: 0.6656 - val_loss: 0.6400 - val_accuracy: 0.6560\n",
            "Epoch 15/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6222 - accuracy: 0.6656 - val_loss: 0.6399 - val_accuracy: 0.6560\n",
            "Epoch 16/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6219 - accuracy: 0.6656 - val_loss: 0.6398 - val_accuracy: 0.6560\n",
            "Epoch 17/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6215 - accuracy: 0.6656 - val_loss: 0.6397 - val_accuracy: 0.6560\n",
            "Epoch 18/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6212 - accuracy: 0.6656 - val_loss: 0.6397 - val_accuracy: 0.6560\n",
            "Epoch 19/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6209 - accuracy: 0.6656 - val_loss: 0.6395 - val_accuracy: 0.6560\n",
            "Epoch 20/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6206 - accuracy: 0.6656 - val_loss: 0.6393 - val_accuracy: 0.6560\n",
            "Epoch 21/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6203 - accuracy: 0.6656 - val_loss: 0.6393 - val_accuracy: 0.6560\n",
            "Epoch 22/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6200 - accuracy: 0.6656 - val_loss: 0.6392 - val_accuracy: 0.6560\n",
            "Epoch 23/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6197 - accuracy: 0.6656 - val_loss: 0.6389 - val_accuracy: 0.6560\n",
            "Epoch 24/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6195 - accuracy: 0.6656 - val_loss: 0.6389 - val_accuracy: 0.6560\n",
            "Epoch 25/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6192 - accuracy: 0.6656 - val_loss: 0.6388 - val_accuracy: 0.6560\n",
            "Epoch 26/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6189 - accuracy: 0.6656 - val_loss: 0.6385 - val_accuracy: 0.6560\n",
            "Epoch 27/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6186 - accuracy: 0.6656 - val_loss: 0.6384 - val_accuracy: 0.6560\n",
            "Epoch 28/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6184 - accuracy: 0.6656 - val_loss: 0.6381 - val_accuracy: 0.6560\n",
            "Epoch 29/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6181 - accuracy: 0.6656 - val_loss: 0.6380 - val_accuracy: 0.6560\n",
            "Epoch 30/30\n",
            "428/428 [==============================] - 2s 4ms/step - loss: 0.6179 - accuracy: 0.6656 - val_loss: 0.6379 - val_accuracy: 0.6560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_classifier.history.history['accuracy'][29]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjUZV1Him_QZ",
        "outputId": "5079197a-4d3c-444a-f479-01340cfb2376"
      },
      "id": "gjUZV1Him_QZ",
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6656197309494019"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ruscorpora_upos_cbow Classifier"
      ],
      "metadata": {
        "id": "DVObCjVQrj49"
      },
      "id": "DVObCjVQrj49"
    },
    {
      "cell_type": "code",
      "source": [
        "rusvec_model = gensim.models.KeyedVectors.load_word2vec_format('ruscorpora_upos_cbow_300_20_2019.bin', binary=True)"
      ],
      "metadata": {
        "id": "P8EtOCR7roI8"
      },
      "id": "P8EtOCR7roI8",
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы взять эмбеддинги из этой модели, нужно сделать препроцессинг с POS-tag разметкой (но на этом я решила остановиться))"
      ],
      "metadata": {
        "id": "1HQYqbnj-I6Q"
      },
      "id": "1HQYqbnj-I6Q"
    },
    {
      "cell_type": "code",
      "source": [
        "for n in rusvec_model.most_similar(positive=[u'школа_NOUN']):\n",
        "    print(n[0], n[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUX7DoKB90-6",
        "outputId": "75950c36-1e27-4f1b-cbb9-aa974b7a6e08"
      },
      "id": "VUX7DoKB90-6",
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "школа_PROPN 0.7144617438316345\n",
            "училище_NOUN 0.6902965307235718\n",
            "гимназия_NOUN 0.6282987594604492\n",
            "общеобразовательный_ADJ 0.6118607521057129\n",
            "интернат_NOUN 0.6072897911071777\n",
            "обучение_NOUN 0.6067934036254883\n",
            "педагог_NOUN 0.6042443513870239\n",
            "колледж_NOUN 0.6035988330841064\n",
            "профтехучилищ_NOUN 0.5997089147567749\n",
            "пту_NOUN 0.5955549478530884\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Copy of hw6_embeddings(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}